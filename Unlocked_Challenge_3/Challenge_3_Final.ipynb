{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e1a917",
   "metadata": {},
   "source": [
    "# Challenge 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e751f",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df59ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import math\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from DataUtils.DownloadCapuchin import download_dataset,parse_datasets\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from pydub import AudioSegment, effects\n",
    "from pydub.generators import WhiteNoise\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e476e",
   "metadata": {},
   "source": [
    "#### NOTE: Pydub requires FFMPEG so make sure you have that installed :\n",
    "* [FFMPEG Setup Guide](https://github.com/jiaaro/pydub#getting-ffmpeg-set-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ff801",
   "metadata": {},
   "source": [
    "## Download Training Data\n",
    "Script that does this is located in `/DataUtils/DownloadCapuchin.py` if you want to look at it or make changes but it should work out of the gate unless you are missing a package install or dont have FFMPEG installed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset()\n",
    "parse_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69011a",
   "metadata": {},
   "source": [
    "### Listen to Parsed Samples\n",
    "If you want to hear a particular sound clip within Jupyter Notebooks you can listen with `display.display(display.Audio(file_path))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "capuchin_files = os.listdir(\"Parsed_Capuchinbird_Clips\")\n",
    "not_capuchin_files = os.listdir(\"Parsed_Not_Capuchinbird_Clips\")\n",
    "Capuchin_File = \"XC3776-6.wav\" #random.choice(capuchin_files)\n",
    "Not_Capuchin_File = random.choice(not_capuchin_files)\n",
    "print(f\"Displaying {Capuchin_File} which is an example of a Parsed Capuchinbird Call:\")\n",
    "display.display(display.Audio(os.path.join(\"Parsed_Capuchinbird_Clips\",Capuchin_File)))\n",
    "print(f\"Displaying {Not_Capuchin_File} which is an example of a Parsed Other Noise:\")\n",
    "display.display(display.Audio(os.path.join(\"Parsed_Not_Capuchinbird_Clips\",Not_Capuchin_File)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b362f",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Similar to Image Recognition in order to produce a robust model you may want to Augment your training set with transformed samples. Here we provide a few examples that you can use to transform audio clips. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ebfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Add_White_Noise(sound, decibels = 50):\n",
    "    \"\"\"\n",
    "    Add White Noise to an Audio Clip and return the new clip\n",
    "    Note: sound should be an AudioSegment\n",
    "    \"\"\"\n",
    "    noise = WhiteNoise().to_audio_segment(duration=len(sound))-decibels\n",
    "    combined = sound.overlay(noise)\n",
    "    return combined\n",
    "def Normalize_Volume(sound):\n",
    "    \"\"\"\n",
    "    Normalize the Volume of a Clip and return the new clip\n",
    "    Note :sound should be an AudioSegment\n",
    "    \"\"\"\n",
    "    normalized_sound = effects.normalize(sound) \n",
    "    return normalized_sound\n",
    "def Filter_Out_High_Frequency(sound,cutoff = 8e3):\n",
    "    \"\"\"\n",
    "    Filter out High Frequencies in a Clip and return the new clip\n",
    "    Note: sound should be an AudioSegment and cutoff is in Hz (default is 8kHz)\n",
    "    \"\"\"\n",
    "    filtered_sound = effects.low_pass_filter(sound,cutoff) \n",
    "    return filtered_sound\n",
    "def Filter_Out_Low_Frequency(sound,cutoff = 8e3):\n",
    "    \"\"\"\n",
    "    Filter out High Frequencies in a Clip and return the new clip\n",
    "    Note: sound should be an AudioSegment and cutoff is in Hz (default is 8kHz)\n",
    "    \"\"\"\n",
    "    filtered_sound = effects.high_pass_filter(sound,cutoff) \n",
    "    return filtered_sound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c762699",
   "metadata": {},
   "source": [
    "## Model Training (Spectrograph and CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088f1b0",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1721321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio(audio_binary):\n",
    "    # Decode WAV-encoded audio files to `float32` tensors, normalized\n",
    "    # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.\n",
    "    audio, _ = tf.audio.decode_wav(contents=audio_binary,desired_channels=1,)\n",
    "    # Since all the data is single channel (mono), drop the `channels`\n",
    "    # axis from the array.\n",
    "    return tf.squeeze(audio, axis=-1)\n",
    "def get_label(file_path):\n",
    "    parts = tf.strings.split(\n",
    "        input=file_path,\n",
    "        sep=os.path.sep)\n",
    "    # Note: You'll use indexing here instead of tuple unpacking to enable this\n",
    "    # to work in a TensorFlow graph.\n",
    "    return parts[-2]\n",
    "def get_waveform_and_label(file_path):\n",
    "    label = get_label(file_path)\n",
    "    audio_binary = tf.io.read_file(file_path)\n",
    "    waveform = decode_audio(audio_binary)\n",
    "    return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4210cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "    # Zero-padding for an audio waveform with less than 16,000 samples.\n",
    "    input_len = 16000\n",
    "    waveform = waveform[:input_len]\n",
    "    zero_padding = tf.zeros(\n",
    "    [16000] - tf.shape(waveform),\n",
    "    dtype=tf.float32)\n",
    "    # Cast the waveform tensors' dtype to float32.\n",
    "    waveform = tf.cast(waveform, dtype=tf.float32)\n",
    "    # Concatenate the waveform with `zero_padding`, which ensures all audio\n",
    "    # clips are of the same length.\n",
    "    equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "    # Convert the waveform to a spectrogram via a STFT.\n",
    "    spectrogram = tf.signal.stft(\n",
    "    equal_length, frame_length=255, frame_step=128)\n",
    "    # Obtain the magnitude of the STFT.\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    # Add a `channels` dimension, so that the spectrogram can be used\n",
    "    # as image-like input data with convolution layers (which expect\n",
    "    # shape (`batch_size`, `height`, `width`, `channels`).\n",
    "    spectrogram = spectrogram[..., tf.newaxis]\n",
    "    return spectrogram\n",
    "def plot_spectrogram(spectrogram, ax):\n",
    "    if len(spectrogram.shape) > 2:\n",
    "        assert len(spectrogram.shape) == 3\n",
    "        spectrogram = np.squeeze(spectrogram, axis=-1)\n",
    "    # Convert the frequencies to log scale and transpose, so that the time is\n",
    "    # represented on the x-axis (columns).\n",
    "    # Add an epsilon to avoid taking a log of zero.\n",
    "    log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
    "    height = log_spec.shape[0]\n",
    "    width = log_spec.shape[1]\n",
    "    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "    Y = range(height)\n",
    "    ax.pcolormesh(X, Y, log_spec)\n",
    "def get_spectrogram_and_label_id(audio, label):\n",
    "    spectrogram = get_spectrogram(audio)\n",
    "    label_id = tf.argmax(label == commands)\n",
    "    return spectrogram, label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b5d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(files):\n",
    "    files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    output_ds = files_ds.map(\n",
    "        map_func=get_waveform_and_label,\n",
    "        num_parallel_calls=AUTOTUNE)\n",
    "    output_ds = output_ds.map(\n",
    "        map_func=get_spectrogram_and_label_id,\n",
    "        num_parallel_calls=AUTOTUNE)\n",
    "    return output_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7cd07",
   "metadata": {},
   "source": [
    "### Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab50b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 1842\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "# Turn off warnings for cleaner looking notebook\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14023785",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd()\n",
    "commands = [\"Parsed_Capuchinbird_Clips\",\"Parsed_Not_Capuchinbird_Clips\"]\n",
    "filenames_Capuchinbird = tf.io.gfile.glob(str(data_dir) + '/Parsed_Capuchinbird_Clips/*')\n",
    "filenames_Not_Capuchinbird = tf.io.gfile.glob(str(data_dir) + '/Parsed_Not_Capuchinbird_Clips/*')\n",
    "filenames =tf.concat([filenames_Capuchinbird, filenames_Not_Capuchinbird], 0)\n",
    "filenames = tf.random.shuffle(filenames)\n",
    "num_samples = len(filenames)\n",
    "print('Number of total examples:', num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36baeb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(.8*num_samples)\n",
    "val_split = int(.1*num_samples)\n",
    "test_split = num_samples - train_split - val_split\n",
    "train_files = filenames[:train_split]\n",
    "val_files = filenames[train_split: train_split + val_split]\n",
    "test_files = filenames[-1*test_split:]\n",
    "\n",
    "print('Training set size', len(train_files))\n",
    "print('Validation set size', len(val_files))\n",
    "print('Test set size', len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946fa49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "\n",
    "waveform_ds = files_ds.map(\n",
    "    map_func=get_waveform_and_label,\n",
    "    num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd34e7f",
   "metadata": {},
   "source": [
    "### Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows * cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
    "\n",
    "for i, (audio, label) in enumerate(waveform_ds.take(n)):\n",
    "    r = i // cols\n",
    "    c = i % cols\n",
    "    ax = axes[r][c]\n",
    "    ax.plot(audio.numpy())\n",
    "    ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "    label = label.numpy().decode('utf-8')\n",
    "    ax.set_title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746dbf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for waveform, label in waveform_ds.take(1):\n",
    "    label = label.numpy().decode('utf-8')\n",
    "    spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "print('Label:', label)\n",
    "print('Waveform shape:', waveform.shape)\n",
    "print('Spectrogram shape:', spectrogram.shape)\n",
    "print('Audio playback')\n",
    "display.display(display.Audio(waveform, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, 16000])\n",
    "\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title('Spectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db24677",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_ds = waveform_ds.map(\n",
    "  map_func=get_spectrogram_and_label_id,\n",
    "  num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78402c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "\n",
    "for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n",
    "    r = i // cols\n",
    "    c = i % cols\n",
    "    ax = axes[r][c]\n",
    "    plot_spectrogram(spectrogram.numpy(), ax)\n",
    "    ax.set_title(commands[label_id.numpy()])\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de699084",
   "metadata": {},
   "source": [
    "### Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = spectrogram_ds\n",
    "val_ds = preprocess_dataset(val_files)\n",
    "test_ds = preprocess_dataset(test_files)\n",
    "\n",
    "batch_size = 64\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe5527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spectrogram, _ in spectrogram_ds.take(1):\n",
    "    input_shape = spectrogram.shape\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(commands)\n",
    "\n",
    "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
    "norm_layer = layers.Normalization()\n",
    "# Fit the state of the layer to the spectrograms\n",
    "# with `Normalization.adapt`.\n",
    "norm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
    "\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    # Downsample the input.\n",
    "    layers.Resizing(32, 32),\n",
    "    # Normalize.\n",
    "    norm_layer,\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_labels),\n",
    "])\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "history = cnn_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, \n",
    "                                               patience=5,\n",
    "                                               restore_best_weights=True\n",
    "                                              ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad4018",
   "metadata": {},
   "source": [
    "### Investigate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c30e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio = []\n",
    "test_labels = []\n",
    "\n",
    "for audio, label in test_ds:\n",
    "    test_audio.append(audio.numpy())\n",
    "    test_labels.append(label.numpy())\n",
    "\n",
    "test_audio = np.array(test_audio)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "y_pred = np.argmax(cnn_model.predict(test_audio), axis=1)\n",
    "y_true = test_labels\n",
    "\n",
    "test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "print(f'Test set accuracy: {test_acc:.0%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df049c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=commands,\n",
    "            yticklabels=commands,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e598ec0",
   "metadata": {},
   "source": [
    "### Example Prediction on file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ae0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "capuchin_files = os.listdir(\"Parsed_Capuchinbird_Clips\")\n",
    "sample_file = random.choice(capuchin_files)\n",
    "print(sample_file)\n",
    "sample_ds = preprocess_dataset([os.path.join(\"Parsed_Capuchinbird_Clips\",sample_file)])\n",
    "\n",
    "for spectrogram, label in sample_ds.batch(1):\n",
    "    prediction = cnn_model(spectrogram)\n",
    "    plt.bar(commands, tf.nn.softmax(prediction[0]))\n",
    "    plt.title(f'Predictions for \"{commands[label[0]]}\"')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a76ba5",
   "metadata": {},
   "source": [
    "## Model Training (Transfer Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad17d9",
   "metadata": {},
   "source": [
    "### Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_wav_16k_mono(filename):\n",
    "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    wav, sample_rate = tf.audio.decode_wav(\n",
    "          file_contents,\n",
    "          desired_channels=1)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return wav\n",
    "def load_wav_for_map(filename, label):\n",
    "    return load_wav_16k_mono(filename), label\n",
    "def extract_embedding(wav_data, label):\n",
    "    \"\"\" run YAMNet to extract embedding from the wav data \"\"\"\n",
    "    scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "    num_embeddings = tf.shape(embeddings)[0]\n",
    "    return (embeddings,\n",
    "            tf.repeat(label, num_embeddings)\n",
    "           )\n",
    "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    \"\"\" Split Train, Test and Validation Datasets out of Dataframe \"\"\"\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    if shuffle:\n",
    "        # Specify seed to always have the same split distribution between runs\n",
    "        ds = ds.shuffle(shuffle_size, seed=1842)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "# Filter out Annoying Tensorflow Warnings\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01533c",
   "metadata": {},
   "source": [
    "### Download YAMNet pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d8109b",
   "metadata": {},
   "source": [
    "### Build Training Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "capuchin_files = os.listdir(\"Parsed_Capuchinbird_Clips\")\n",
    "capuchin = []\n",
    "for file in capuchin_files:\n",
    "    if file.endswith('.wav'):\n",
    "        capuchin.append(os.path.join(\"Parsed_Capuchinbird_Clips\",file))\n",
    "not_capuchin_files = os.listdir(\"Parsed_Not_Capuchinbird_Clips\")\n",
    "not_capuchin = []\n",
    "for file in not_capuchin_files:\n",
    "    if file.endswith('.wav'):\n",
    "        not_capuchin.append(os.path.join(\"Parsed_Not_Capuchinbird_Clips\",file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ed121",
   "metadata": {},
   "outputs": [],
   "source": [
    "capuchin_pd = pd.DataFrame({\"filename\":capuchin,\"target\":1})\n",
    "not_capuchin_pd = pd.DataFrame({\"filename\":not_capuchin,\"target\":0})\n",
    "dataset_pd = pd.concat([capuchin_pd,not_capuchin_pd],axis=0,ignore_index=True)\n",
    "dataset_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e1e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ds = tf.data.Dataset.from_tensor_slices((dataset_pd[\"filename\"], dataset_pd[\"target\"]))\n",
    "main_ds = main_ds.map(load_wav_for_map)\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract embedding\n",
    "main_ds = main_ds.map(extract_embedding).unbatch()\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659dc797",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(main_ds,len(dataset_pd))\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fdcd2",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d05f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_transfer_learning_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(2)\n",
    "], name='yamnet_transfer_learning_model')\n",
    "\n",
    "yamnet_transfer_learning_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_transfer_learning_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7091d",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = yamnet_transfer_learning_model.fit(train_ds,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce0f21",
   "metadata": {},
   "source": [
    "### Investigate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bab1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = yamnet_transfer_learning_model.evaluate(test_ds)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18f8d8",
   "metadata": {},
   "source": [
    "## Use Model Outputs to Count Capuchinbird Calls\n",
    "Now that we have a working model lets extend it to count Capuchinbird Calls and test out how well it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f2a1e",
   "metadata": {},
   "source": [
    "### Build Simple Test Case Generator\n",
    "You can Improve tests to use a variety of calls for Capuchin and Not Capuchin from the test set or other clips you gather or you can mix your own audio clips in another way and test them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c835af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def locations_to_approx_seconds(location):\n",
    "    return str(location*3)\n",
    "def locations_to_approx_result_row(location):\n",
    "    return str(int(location*6.5))\n",
    "def make_tests(capuchin_path,not_capuchin_path,capuchin_count):\n",
    "    seconds = 1000\n",
    "    capuchin_sound = AudioSegment.from_wav(capuchin_path)\n",
    "    not_capuchin_sound = AudioSegment.from_wav(not_capuchin_path)\n",
    "    total_clips = 60\n",
    "    locations = random.sample(range(1, total_clips), capuchin_count)\n",
    "    locations.sort()\n",
    "    clip_positions = \",\".join(map(str,locations))\n",
    "    approx_locations_sec = \",\".join(map(locations_to_approx_seconds,locations))\n",
    "    approx_locations_result_row = \",\".join(map(locations_to_approx_result_row,locations))\n",
    "    print(f\"Capuchin Calls are Located at [{clip_positions}] positions in the clip\")\n",
    "    print(f\"Capuchin Calls are Located around [{approx_locations_sec}] seconds in the clip\")\n",
    "    print(f\"Capuchin Calls are Located around [{approx_locations_result_row}] in the result rows\")\n",
    "    clips = []\n",
    "    for i in range(total_clips):\n",
    "        if i in locations:\n",
    "            clips.append(capuchin_sound)\n",
    "        else:\n",
    "            test = random.sample([0,1],1)\n",
    "            if test == 0:\n",
    "                clips.append(WhiteNoise().to_audio_segment(duration=len(3*1000)))\n",
    "            else:\n",
    "                clips.append(not_capuchin_sound)\n",
    "    final_clip = clips[0]\n",
    "    for i in range(1,len(clips)):\n",
    "        final_clip = final_clip + clips[i]\n",
    "    output_file = \"test.wav\"\n",
    "    final_clip.export(output_file, format=\"wav\")\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c5d96",
   "metadata": {},
   "source": [
    "### Generate Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb7493",
   "metadata": {},
   "outputs": [],
   "source": [
    "capuchin_files = os.listdir(\"Parsed_Capuchinbird_Clips\")\n",
    "not_capuchin_files = os.listdir(\"Parsed_Not_Capuchinbird_Clips\")\n",
    "Capuchin_File = random.choice(capuchin_files)\n",
    "Not_Capuchin_File = random.choice(not_capuchin_files)\n",
    "Num_Capuchin_Calls = 5\n",
    "print(f\"Using {Capuchin_File} and {Not_Capuchin_File} to generate {Num_Capuchin_Calls} Capuchinbird Calls\")\n",
    "\n",
    "not_capuchin_path = os.path.join(\"Parsed_Not_Capuchinbird_Clips\",Not_Capuchin_File)\n",
    "capuchin_path = os.path.join(\"Parsed_Capuchinbird_Clips\",Capuchin_File)\n",
    "test_file_name = make_tests(capuchin_path,not_capuchin_path,Num_Capuchin_Calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faf74fe",
   "metadata": {},
   "source": [
    "### Generate Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4247a41d",
   "metadata": {},
   "source": [
    "#### Illustration Gif from Tensorflow website on how YAMNet works\n",
    "\n",
    "![](https://1.bp.blogspot.com/-CLyq7ilQIow/YDawZXp_NiI/AAAAAAAAEEg/vVa58jb24Fkw-LZPsezB_qMdnvndOYuzwCLcBGAsYHQ/s0/yamnet_animation%2B%25282%2529.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f34c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_wav_data = load_wav_16k_mono(test_file_name)\n",
    "scores, embeddings, spectrogram = yamnet_model(testing_wav_data)\n",
    "result = yamnet_transfer_learning_model(embeddings).numpy()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09b6cd1",
   "metadata": {},
   "source": [
    "### Simple Capuchinbird Call Counter\n",
    "This code is counting all Positive Model Scores for the Positive Class (Capuchinbird) that occur in a row as a single call. It is easy to see cases where this will fail to properly count the calls so building a more complex Call Counter will be up to you and an important piece of your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375fcbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "previous_pos = 0\n",
    "capuchin_count = 0\n",
    "print(\"Embeddings with Positive Val for Capuchinbird Call:\")\n",
    "for row in result:\n",
    "    if row[1]>0:\n",
    "        if count - previous_pos > 1:\n",
    "            capuchin_count += 1\n",
    "        previous_pos = count\n",
    "        value = '%.2f' % round(row[1],2)\n",
    "        if count <100:\n",
    "            print(f\"  row:  {count} value: {value}\")\n",
    "        else:\n",
    "            print(f\"  row: {count} value: {value}\")\n",
    "    count += 1\n",
    "print(f\"Found {capuchin_count} of {Num_Capuchin_Calls} Capuchin Calls!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3dba5",
   "metadata": {},
   "source": [
    "## Sample Output File\n",
    "Your submission should consist of a csv with scores for all the clips in the `Forest Recordings` Folder and should have two columns: clip_name and call_count. There should be 100 audio clips so make sure you have scores for clips 0-99!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e09bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_names = os.listdir(\"Forest Recordings\")\n",
    "clip_names.sort()\n",
    "counts = [0]*len(clip_names) # Change with your scores\n",
    "submission_df = pd.DataFrame({\"clip_name\":clip_names,\"call_count\":counts})\n",
    "submission_df.to_csv(\"submission.csv\",index=False)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e756ad1",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "* Make a Capuchinbird Call Count for each clip in the `Forest Recordings` Folder\n",
    "* Iterate Model/Training Set/Call Counter to Improve Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47dead6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
